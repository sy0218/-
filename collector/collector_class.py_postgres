#!/usr/bin/python3
from conf.config_log import setup_logger
from common.job_class import Get_env, Get_properties, StopChecker, DataPreProcessor
from common.crawling_class import ChromeDriver, JobParser
from common.hook_class import RedisHook, KafkaHook, PostgresHook

import psycopg2
from psycopg2.extras import execute_values

import sys, time
logger = setup_logger(__name__)

def _main():
    """
    메인 수집 로직
    - 웹 크롤링
    - Redis 중복 체크
    - Kafka Avro Producer 전송
    """

    def _get_xpaths(domain, job_type):
        """
        도메인 + 직무 타입별 XPath 정보 반환
        """
        return {
            "response": properties["xpath"][f"{domain}.response.{job_type}"],
            "href": properties["xpath"][f"{domain}.href.{job_type}"],
            "company": properties["xpath"][f"{domain}.company.{job_type}"],
            "title": properties["xpath"][f"{domain}.title.{job_type}"],
            "wait": properties["xpath"][f"{domain}.wait.{job_type}"]
        }

    try:
        # ===============================
        # 환경 변수 및 설정 로드
        # ===============================
        collector_env = Get_env._collector()
        redis_env = Get_env._redis()
        kafka_env = Get_env._kafka()
        pg_env = Get_env._postgres()
        properties = Get_properties(collector_env["config_path"])

        logger.info("환경 변수 및 설정 로드 완료")

        # ===============================
        # Redis 연결 (중복 데이터 체크용)
        # ===============================
        redis = RedisHook(
            redis_env["redis_host"],
            redis_env["redis_port"],
            redis_env["redis_db"],
            redis_env["redis_password"]
        )
        redis.connect()
        logger.info("Redis 연결 완료")

        # ===============================
        # Kafka + Schema Registry 연결
        # ===============================
        kafka = KafkaHook(kafka_env["kafka_host"])
        kafka.avro_connect(
            kafka_env["schema_registry"],
            properties["schema"]["job_header"]
        )
        logger.info("Kafka Avro Producer 연결 완료")

        # ===============================
        # Postgresql 연결 (중복 데이터 체크용)
        # ===============================
        postgresql = PostgresHook(
            pg_env["pg_host"],
            pg_env["pg_port"],
            pg_env["pg_db"],
            pg_env["pg_user"],
            pg_env["pg_password"]
        )
        postgresql.connect()
        logger.info("PostgreSQL 연결 완료")

        # ===============================
        # Chrome 브라우저 기동
        # ===============================
        browser = ChromeDriver()
        logger.info("ChromeDriver 시작")

        domain_lst = properties["domain"]["catagory"].split(',')
        job_count = int(properties["job_catagory_count"]["count"])

        # ===============================
        # 메인 수집 루프
        # ===============================
        while True:
            for domain in domain_lst:
                for number in range(1, job_count + 1):

                    job_type = properties["url_number"][f"url{number}"]
                    job_url = properties["url"][f"{domain}.url.{job_type}"]
                    xpaths = _get_xpaths(domain, job_type)

                    logger.info(f"수집 시작 | domain={domain}, job_type={job_type}")
                    logger.debug(f"접속 URL: {job_url}")

                    # 페이지 접속 및 스크롤
                    browser.get(job_url)
                    browser.wait_css(xpaths["wait"], 10)
                    browser.autoscroll(xpaths["wait"], 10, 1, 3)

                    parser = JobParser(browser)
                    response = parser.get_response()

                    job_hash_map = []
                    # ===============================
                    # 채용 공고 파싱
                    # ===============================
                    for job_html in response.xpath(xpaths["response"]):
                        job_header = parser.get_job(
                            domain,
                            job_html,
                            xpaths["href"],
                            xpaths["company"],
                            xpaths["title"]
                        )

                        # href 기준 해시 (중복 판별 키)
                        href_hash = DataPreProcessor._hash(job_header["href"] + job_header["title"])
                        job_hash_map.append((href_hash, job_header))


                    if job_hash_map:
                        with postgresql.conn.cursor() as cur:
                            values = [(r[0],) for r in job_hash_map]
                            sql = """
                            INSERT INTO job.job_set (job_set)
                            VALUES %s
                            ON CONFLICT (job_set) DO NOTHING
                            RETURNING job_set
                            """
                            inserted_rows = execute_values(cur, sql, values, fetch=True)
                            postgresql.conn.commit()

                            # 삽입된 hash set
                            inserted_hashes = set(r[0] for r in inserted_rows)

                            for href_hash, job_header in job_hash_map:
                                if href_hash in inserted_hashes:
                                    logger.info(
                                        f"Kafka 전송 완료 | topic={kafka_env['job_topic']} | company={job_header['company']} | title={job_header['title']}"
                                    )
                                else:
                                    logger.debug(
                                        f"중복 데이터 스킵 (PostgreSQL) | href={job_header['href']}"
                                    )
 
                    # ===============================
                    # 종료 플래그 체크
                    # ===============================
                    if StopChecker._job_stop(
                        collector_env["stop_dir"],
                        collector_env["stop_file"]
                    ):
                        logger.warning("Stop 파일 감지 → Collector 종료")
                        sys.exit(0)

                    time.sleep(3)

    except Exception as e:
        logger.error("Collector 실행 중 오류 발생", exc_info=True)

    finally:
        # ===============================
        # 리소스 정리
        # ===============================
        browser.quit()
        redis.close()
        postgresql.close()
        kafka.flush()
        logger.info("Collector 정상 종료")


if __name__ == "__main__":
    _main()
